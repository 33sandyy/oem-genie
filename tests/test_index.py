import os
from langchain.schema import Document
from src.data_ingestion import PDFIngestor
from src.chunker import TextChunker
from src.embedder import Embedder
from src.vectorstore_chroma import ChromaVectorStore

def test_build_index(pdf_folder="data", persist_dir="chroma_store_test"):
    # 1Ô∏è‚É£ Ingest PDFs
    ingestor = PDFIngestor(pdf_folder)
    raw_docs = ingestor.ingest_folder()

    # 2Ô∏è‚É£ Chunk text
    chunker = TextChunker(chunk_size=500, overlap=100)
    chunk_dicts = chunker.create_chunks_from_docs(raw_docs)

    # 3Ô∏è‚É£ Convert to LangChain Documents with safe metadata
    documents = [
        Document(
            page_content=chunk["text"],
            metadata={
                "id": chunk["id"],
                "source": chunk["source"],
                "page_number": chunk["page_number"],
                "figures": ", ".join(chunk.get("figures", [])) if chunk.get("figures") else None,
            },
        )
        for chunk in chunk_dicts
    ]

    print(f"‚úÖ Created {len(documents)} chunks from {len(raw_docs)} PDFs.")

    # 4Ô∏è‚É£ Embedding
    embedder = Embedder()

    # 5Ô∏è‚É£ Build test Chroma store
    chroma = ChromaVectorStore(persist_dir)
    vectordb = chroma.create_collection_from_chunks(chunks=documents, embedding=embedder.embedder, persist=True)

    print(f"‚úÖ Test Chroma store built at {persist_dir}")
    print("üì¶ Docs inside:", vectordb._collection.count())

if __name__ == "__main__":
    test_build_index()